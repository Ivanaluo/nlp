# NLP

## blog

Jay Alammar’s illustrated blog
[here](http://jalammar.github.io/)

Sebastian Ruder’s blog
[here](http://ruder.io/)

NLP Highlights hosted by Matt Gardner and Waleed Ammar
[here](https://podcasts.apple.com/us/podcast/nlp-highlights/id1235937471)


## reviews
Neural Text Generation: Past, Present and Beyond
[here](https://arxiv.org/pdf/1803.07133.pdf)

Analysis Methods in Neural Language Processing: A Survey
[here](https://arxiv.org/pdf/1812.08951.pdf)


## attention
* 论文《Attention is all you need》[here](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
	* 包含了对 Transformer 的介绍 [论文视频](https://www.youtube.com/watch?v=iDulhoQ2pro)

* 博文《The Annotated Transformer》[here](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
	* 使用 Pytorch 对 Transformer 进行复现的文章，由 HarvardNLP 发布 [模型视频](https://www.youtube.com/watch?v=rBCqOTEfxvg)
	* Google 发布的 attention 机制官方介绍 [视频](https://www.youtube.com/watch?v=rBCqOTEfxvg)
	* Google AI 对 Transformer 的介绍 [博文](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)

* 论文《Transformer-XL:Attentive Language Models Beyond a Fixed Length Context paper》[here](https://arxiv.org/pdf/1901.02860.pdf)
	* 该模型对 Transformer 进行了改进，但这一改进没有被 BERT 采用
	* Google 对 Transformer-XL 的介绍 [博文](https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html)
	* Transformer-XL 的介绍《Transformer-XL — CombiningTransformers and RNNs Into a State-of-the-art Language Model》[博文](https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/)//[视频](https://www.youtube.com/watch?v=cXZ9YBqH3m0)
	
* 博文《The IllustratedTransformer》
[here](http://jalammar.github.io/illustrated-transformer/)

* 博文《Attention and Memoryin Deep Learning and NLP》[here](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)
	* 一篇对注意力机制进行介绍的博文

* 博文《Attention and Augmented Recurrent Neural Networks blog》 [here](https://distill.pub/2016/augmented-rnns/)
	* 对注意力机制和正则化循环神经网络进行了介绍

* 代码 使用一个简单的 Transformer 模型进行序列标注 [here](https://medium.com/@kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-i-a1815655cd8) 

